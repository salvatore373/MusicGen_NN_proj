{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7Chf-jlOW3Dd",
        "sO2f9bK0b81p",
        "4-5ToT4BklsY",
        "BITGhxmVqxEn",
        "Hi2LhkNuW01S",
        "gmtnGAz9W6_V",
        "VuSm6LU5XACt",
        "2IPP0jXnpz43"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Salvatore Michele Rago, Mattia Maffongelli: MusicGen_NN"
      ],
      "metadata": {
        "id": "CddxT5FrURlM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this Google Colab Notebook, we continue the study of the of the paper \"***MusicGen, a simple and controllable model for music generation***\", commented yet in the README file.\n",
        "\n",
        "As previously explained, MusicGen is an **auto-regressive transformer-based decoder** designed to create a framework capable of generating music conditioned on both specific **textual input** and **chromagrams** extracted from a set of pre-existing melodies.\n",
        "\n",
        "The model primarily relies on its transformer decoder structure, which we will delve into later. However, this decoder cannot do the right things without the correct inputs. Therefore, to achieve functionality, it was necessary to work on **other aspects**, such as **tokenizing input audio** files, **tokenizing input text** through a pre-trained encoder, and **extracting/processing the chromagram** of a specific melody. Once the code related to these components was designed and implemented, we proceeded to the actual development of the decoder. This involved constructing its structure and incorporating the **\"delay pattern\"** related to the **codebooks** and tokens passed as input."
      ],
      "metadata": {
        "id": "cxtWHgsbUZnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio tokenization - EnCodec\n",
        "\n",
        "Let's start talking about the audio files tokenization.\n",
        "The code of this part is implemented in the \"*Encodec.py*\" file, which constructs a new class: **AudioTokenizer**.\n",
        "\n",
        "First, let's import the necessary libraries for the class:"
      ],
      "metadata": {
        "id": "7Chf-jlOW3Dd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFk8X27zDbQp"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Audio\n",
        "from transformers import EncodecModel, AutoProcessor\n",
        "import soundfile as sf\n",
        "import librosa"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned earlier, for the construction of this class, we utilized a pre-trained model, namely **EnCodec**. To use it, two functions need to be called from two different classes: **EncodecModel** and **AutoProcessor**. These functions are crucial for generating audio tokens. As can be observed, we opted for a 24kHz version rather than 32kHz. Despite being somewhat **limited** in capturing audio details, given the computational resources at our disposal, we found it to be the **better choice**.\n",
        "\n",
        "Subsequently, we included two main functions within the class: the first one is ***get_tokens_from_file***, which, using the read method from the soundfile library, returns both the **audio vector** and its **sampling rate** given the file path. Before feeding the audio to the processor, we apply necessary transformations, particularly concerning the **number of channels** (converting from multi-channel to **mono-channel** using the mean of all channels) and the sampling rate (adjusting it to that of the processor). Once this is done, we pass the audio to the processor and then to the model's encoder, allowing us to generate tokens, which are stored in the **\"audio_codes\"** attribute.\n",
        "\n",
        "Finally, the second function, ***save_tokens_to_audio_file***, simply takes the input tokens, processes them in the model's **decoder**, and generates the **reconstructed audio** file, which is then saved."
      ],
      "metadata": {
        "id": "AmNIK-Y_Zkkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioTokenizer:\n",
        "    def __init__(self):\n",
        "        self.model = EncodecModel.from_pretrained(\"facebook/encodec_24khz\")\n",
        "        self.processor = AutoProcessor.from_pretrained(\"facebook/encodec_24khz\")\n",
        "\n",
        "    def get_tokens_from_file(self, file_path):\n",
        "        # Load the audio:\n",
        "        audio_sample, sample_rate = sf.read(file_path)\n",
        "\n",
        "        # Convert from multi-channel to mono-channel with the mean:\n",
        "        if len(audio_sample.shape) > 1:\n",
        "            # Se l'audio Ã¨ stereo o multicanale, calcola la media dei canali per ottenere un segnale mono\n",
        "            audio_sample = audio_sample.mean(axis=1)\n",
        "\n",
        "        # Resampling:\n",
        "        if sample_rate != self.processor.sampling_rate:\n",
        "            audio_sample = librosa.resample(audio_sample, orig_sr=sample_rate, target_sr=self.processor.sampling_rate)\n",
        "\n",
        "        inputs = self.processor(raw_audio=audio_sample, sampling_rate=self.processor.sampling_rate, return_tensors=\"pt\")\n",
        "\n",
        "        encoder_outputs = self.model.encode(input_values=inputs[\"input_values\"],\n",
        "                                            padding_mask=inputs.get(\"attention_mask\", None),\n",
        "                                            bandwidth=3.0)\n",
        "\n",
        "        # Take the tokens with the attribute audio_codes\n",
        "        tokens = encoder_outputs.audio_codes\n",
        "        return tokens[0][0]\n",
        "\n",
        "    @staticmethod\n",
        "    def perform_quantization(tokens):\n",
        "        outmap_min, _ = torch.min(tokens, dim=0, keepdim=True)\n",
        "        outmap_max, _ = torch.max(tokens, dim=0, keepdim=True)\n",
        "        normalized_audio_tokens = (tokens - outmap_min) / (outmap_max - outmap_min)  # Broadcasting rules apply\n",
        "\n",
        "        # Define the number of quantization levels (bins)\n",
        "        num_bins = 1024\n",
        "        return (normalized_audio_tokens * (num_bins - 1)).to(torch.int32)\n",
        "\n",
        "    def save_tokens_to_audio_file(self, tokens, output_file_path):\n",
        "        tokens = AudioTokenizer.perform_quantization(tokens)\n",
        "        tokens = tokens.unsqueeze(dim=0).unsqueeze(dim=0)\n",
        "        audio_values = self.model.decode(tokens, [None], None)[0]\n",
        "\n",
        "        # Convert the tensor to obtain a correct file audio wav\n",
        "        reconstructed_audio = audio_values.detach().numpy().flatten()\n",
        "        reconstructed_audio = reconstructed_audio * (2 ** 15)  # Scale the audio\n",
        "\n",
        "        # Save the audio\n",
        "        sf.write(output_file_path, reconstructed_audio, self.processor.sampling_rate)\n"
      ],
      "metadata": {
        "id": "psjkfNPiZqdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What has just been presented is nothing other than the main \"ingredient\" for constructing the transformer decoder!\n",
        "Now let's talk about the **conditioning tensors**."
      ],
      "metadata": {
        "id": "TSUgyC7Zb2j7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FIRST CONDITIONING - TEXT"
      ],
      "metadata": {
        "id": "sO2f9bK0b81p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first conditioning phase involves **textual inputs** corresponding to a textual description of the input audio. What we have implemented is the generation of a **conditioning tensor** C, given as input one or more texts. It has dimensions **T_C** = length of the input sequence **X D**, where D represents the **embedding size**, which in our case is 4 (token size).\n",
        "All of these is represented by the class: **TextToTokenConverter**.\n",
        "\n",
        "In the studied paper, three general approaches were mentioned for this part. The one we chose involves using the pre-trained **T5 model**, with the specific function of generating tokens from a string or text. In this case, the necessary libraries were:"
      ],
      "metadata": {
        "id": "qhYnbW9ecB2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ],
      "metadata": {
        "id": "GqBeSe9KbphX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As for the previous part, here too, we used both the actual model and a kind of processor, namely the **tokenizer**. Among the various options, we decided to use \"***t5_base***\", which comprises 220 million parameters, as we deemed it sufficient for our type of work. The other two versions available were: \"*small*\" with 60 million parameters, and \"*large*\" with 770 million parameters, a number that is actually disproportionate for our resources.\n",
        "\n",
        "At this point, we pass our textual sequence to the tokenizer, which returns a sequence of token identifiers corresponding to the provided text in the form of a **torch tensor**. This tensor is then given as input to the model's encoder, all within the **no_grad condition**, as we don't want to compute and store gradients during the **backward pass** to speed up the execution (and since it's pre-trained). From this, we access the ***last_hidden_state*** containing the final hidden output of the model's encoder layer.\n",
        "\n",
        "The final instruction that this function performs is to adjust the **dimensions** of the obtained conditioning tensor to the dimensions that we actually need as input for the transformer. To do this, we use a **linear layer** that, aware of the potential loss of information, returns a tensor with dimensions **(number of sequences, number of tokens, token size)**."
      ],
      "metadata": {
        "id": "UFXNgh5Edjyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextToTokenConverter:\n",
        "    def __init__(self):\n",
        "        # Since we want the token's dimension to be 4, I use a linear function that reduce the dimension.\n",
        "        self.linear_layer = torch.nn.Linear(768, 4)\n",
        "        self.linear_layer.requires_grad_(False)\n",
        "\n",
        "    def convert_text_to_tokens(self, text):\n",
        "        tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "        model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "\n",
        "        # ENCODER PART WITH T5-MODEL\n",
        "        input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "        with torch.no_grad():\n",
        "            encoder_output = model.encoder(input_ids).last_hidden_state\n",
        "\n",
        "        # Generate the conditioning tensor, having as dimension T_C * D\n",
        "        # where T_C = length of the sequence and D = embedding size.\n",
        "        conditioning_tensor = self.linear_layer(encoder_output)\n",
        "\n",
        "        return conditioning_tensor[0]"
      ],
      "metadata": {
        "id": "OkaPCy_hgTAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defined the method to obtain tokens from an input text, let's focus on the melody conditioning."
      ],
      "metadata": {
        "id": "IjOMHF54kbAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SECOND CONDITIONING - MELODY"
      ],
      "metadata": {
        "id": "4-5ToT4BklsY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While conditioning from textual input is more common, the foundation of a song or music lies in its melody. Therefore, it should be possible to generate new music conditioned on an existing **melodic structure**. To achieve this, the paper proposes **joint conditioning** between text and melody, taking both the conditioning tensor related to the text and the chromagram of a melody as input. The chromagram is filtered through an **information bottleneck** concerning the dominant bin.\n",
        "\n",
        "This was one of the more **challenging** parts for various reasons: on one hand, the paper didn't fully elaborate on how to precisely implement such conditioning, and on the other hand, we faced a **lack of data** to perform training. For these reasons, we decided to implement this part and to incorporate it into the final transformer, albeit without training the corresponding model. Therefore, the transformer functions without the ability to condition on a melody.\n",
        "\n",
        "Nevertheless, it is useful and important to understand how we decided to implement the extraction of the chromagram from an audio file. First, the necessary libraries were:"
      ],
      "metadata": {
        "id": "e3xa1nDTldWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from librosa import filters"
      ],
      "metadata": {
        "id": "S8AJgkSZm1FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case as well, we created a class that encapsulates all the necessary methods: **MelodyToTokenConverter**. Inside, the two main methods are: ***convert_text_melody_to_tokens***, where, similar to audio tokenization, we open the audio file using the soundfile library given its path to obtain both the vector containing the audio and its sampling rate. We convert the audio, if necessary, to mono-channel and finally call the second method ***convert***. This method takes the audio vector, its sampling rate, and the embedding size as input. The first thing it does is create the **spectrogram** using a method from the torchaudio library, setting the number of samples for the Fourier transform (**n_fft**) to 128, and the **normalized** attribute to true, thus normalizing the result of the transform. Subsequently, a **chroma filter matrix** is created to be applied to the tensor containing the spectrogram using the **Einstein summation** operation. This operation allows the generation of the **\"raw\" chromagram** from the spectrogram, which is finally normalized as indicated in the paper, obtaining the actual chromagram of the melody."
      ],
      "metadata": {
        "id": "84CCyXHJnG2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MelodyToTokenConverter:\n",
        "\n",
        "    def convert(self, audio_time_series, sr, embedding_size, argmax=False):\n",
        "        audio_time_series = audio_time_series.to(torch.float)\n",
        "        nfft = 128\n",
        "        spec = torchaudio.transforms.Spectrogram(n_fft=nfft, power=2, center=True,\n",
        "                                                 pad=0, normalized=True)\n",
        "        from librosa import filters\n",
        "        fbanks = torch.from_numpy(filters.chroma(sr=sr, n_fft=nfft, n_chroma=embedding_size))\n",
        "\n",
        "        spec = spec(torch.transpose(audio_time_series, 0, 1)).squeeze(1)\n",
        "        raw_chroma = torch.einsum('cf,...ft->...ct', fbanks, spec)\n",
        "        norm_chroma = torch.nn.functional.normalize(raw_chroma, dim=-2, eps=1e-6)\n",
        "        from einops import rearrange\n",
        "        norm_chroma = rearrange(norm_chroma, 'b d t -> b t d')\n",
        "\n",
        "        if argmax:\n",
        "            idx = norm_chroma.argmax(-1, keepdim=True)\n",
        "            norm_chroma[:] = 0\n",
        "            norm_chroma.scatter_(dim=-1, index=idx, value=1)\n",
        "\n",
        "        return norm_chroma\n",
        "\n",
        "    def convert_text_melody_to_tokens(self, audio_file_path, embedding_size):\n",
        "        y, sr = sf.read(audio_file_path)\n",
        "        if len(y.shape) > 1:  # convert to mono from stereo\n",
        "            y = y.mean(axis=1).reshape(-1, 1)\n",
        "        res = self.convert(torch.from_numpy(y), sr, embedding_size)\n",
        "        return res[0]\n"
      ],
      "metadata": {
        "id": "BolbK5z8qYoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have everything, we can finally talk about the transformer."
      ],
      "metadata": {
        "id": "yiNDKTFqqyQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TRANSFORMER"
      ],
      "metadata": {
        "id": "BITGhxmVqxEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The architecture"
      ],
      "metadata": {
        "id": "Hi2LhkNuW01S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from math import sqrt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from T5_encoder import TextToTokenConverter\n",
        "\n",
        "\n",
        "class MaskedSelfAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 q_param,\n",
        "                 v_param):\n",
        "        \"\"\"\n",
        "        Builds a block that computes masked self-attention.\n",
        "        :param q_param: The not-fixed dimension of the Query, Key matrices\n",
        "        :param v_param: The not-fixed dimension of the Value matrix\n",
        "        \"\"\"\n",
        "        super(MaskedSelfAttention, self).__init__()\n",
        "\n",
        "        self.q_param = q_param\n",
        "        self.v_param = v_param\n",
        "\n",
        "        # The first dim of query, key and value Linear should be the embedding dimension.\n",
        "        # Assuming for simplicity that it is equal to q and v\n",
        "\n",
        "        # considering a X of dimension (n, d):\n",
        "        self.query = nn.Linear(q_param, q_param)  # output dim (n ,q)\n",
        "        self.key = nn.Linear(q_param, q_param)  # output dim (n ,q)\n",
        "        self.value = nn.Linear(q_param, v_param)  # output dim (n ,v)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"\"\"\n",
        "        Compute masked self-attention for the given tuple of Query, Key and Value\n",
        "        :param query\n",
        "        :param key\n",
        "        :param value\n",
        "        :param mask: The mask used in the masked-attention formula. If not given, a matrix of ones will be used.\n",
        "        :return: The computed masked self-attention.\n",
        "        \"\"\"\n",
        "\n",
        "        # Compute the values for query, key and value using learned params\n",
        "        query = self.query(query)\n",
        "        key = self.key(key)\n",
        "        value = self.value(value)\n",
        "\n",
        "        # Compute Q * K^T\n",
        "        query_key = torch.matmul(query, key.t())\n",
        "\n",
        "        # Add masking\n",
        "        if mask is None:\n",
        "            mask = torch.ones(query_key.size())\n",
        "        masked = torch.mul(query_key, mask)\n",
        "\n",
        "        # Compute the attention (with softmax along rows)\n",
        "        h = torch.softmax(masked / sqrt(self.q_param), dim=1)\n",
        "        return torch.matmul(h, value)\n",
        "\n",
        "\n",
        "class CrossAttention(MaskedSelfAttention):\n",
        "    \"\"\"\n",
        "    A block that computes cross-attention.\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, s1, s2):\n",
        "        \"\"\"\n",
        "        Compute the cross-attention for the given input sequences\n",
        "        :param s1: The sequence to use as query.\n",
        "        :param s2: The sequence to use as key and value.\n",
        "        :return: The cross-attention for the given input sequences.\n",
        "        \"\"\"\n",
        "        return super().forward(s1, s2, s2)\n",
        "\n",
        "\n",
        "class MaskedMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, h_param, q_param, v_param, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Builds a block that computes masked self-attention a number of times equal to h_param.\n",
        "        :param h_param: The number of times that masked self-attention should be performed.\n",
        "        :param q_param: The not-fixed dimension of the Query, Key matrices\n",
        "        :param v_param: The not-fixed dimension of the Value matrix\n",
        "        \"\"\"\n",
        "        super(MaskedMultiHeadAttention, self).__init__()\n",
        "        self.h_param = h_param\n",
        "        self.q_param = q_param\n",
        "        self.v_param = v_param\n",
        "\n",
        "        self.attention_layers = [MaskedSelfAttention(q_param, v_param) for _ in range(h_param)]\n",
        "\n",
        "        # Output linear layer (as indicated in Fig. 2 of \"Attention is all you need\")\n",
        "        self.output_lin = nn.Linear(h_param * v_param, v_param)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # Compute the self-attention h times\n",
        "        attentions = [self_att_layer(query, key, value, mask)\n",
        "                      for self_att_layer in self.attention_layers]\n",
        "        # Concatenate the result of each self-attention and then project them\n",
        "        concatenation = torch.cat(attentions, dim=1)\n",
        "        return self.output_lin(concatenation)\n",
        "\n",
        "\n",
        "class MultiHeadCrossAttention(nn.Module):\n",
        "    def __init__(self, h_param, q_param, v_param, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Builds a block that computes masked self-attention a number of times equal to h_param.\n",
        "        :param h_param: The number of times that masked self-attention should be performed.\n",
        "        :param q_param: The not-fixed dimension of the Query, Key matrices\n",
        "        :param v_param: The not-fixed dimension of the Value matrix\n",
        "        \"\"\"\n",
        "        super(MultiHeadCrossAttention, self).__init__()\n",
        "        self.h_param = h_param\n",
        "        self.q_param = q_param\n",
        "        self.v_param = v_param\n",
        "\n",
        "        self.attention_layers = [CrossAttention(q_param, v_param) for _ in range(h_param)]\n",
        "\n",
        "        # Output linear layer (as indicated in Fig. 2 of \"Attention is all you need\")\n",
        "        self.output_lin = nn.Linear(h_param * v_param, v_param)\n",
        "\n",
        "    def forward(self, s1, s2):\n",
        "        # Compute the self-attention h times\n",
        "        attentions = [cross_att_layer(s1, s2)\n",
        "                      for cross_att_layer in self.attention_layers]\n",
        "        # Concatenate the result of each self-attention and then project them\n",
        "        concatenation = torch.cat(attentions, dim=1)\n",
        "        return self.output_lin(concatenation)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, q_val: int, v_val: int, h_val: int, dropout):\n",
        "        \"\"\"\n",
        "        Builds a block of the encoder part of the transformer\n",
        "        :param q_val: The q parameter of the Multi-Head-Attention block.\n",
        "        :param v_val: The v parameter of the Multi-Head-Attention block.\n",
        "        :param h_val: The v parameter of the Multi-Head-Attention block.\n",
        "        :param dropout: The percentage of dropout to place after the normalization\n",
        "        \"\"\"\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.attention = MaskedMultiHeadAttention(h_val, q_val, v_val)\n",
        "        self.normalization_1 = nn.LayerNorm(v_val)\n",
        "\n",
        "        # Define the FeedForward network as stated in section 3.3 of \"Attention is all you need\"\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(v_val, 4 * v_val),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * v_val, v_val),\n",
        "        )\n",
        "\n",
        "        self.normalization_2 = nn.LayerNorm(v_val)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        # Compute the masked self-attention\n",
        "        attention = self.attention(query, key, value)\n",
        "        # Make layer normalization (with residual connection)\n",
        "        first_normalization = self.dropout(self.normalization_1(attention + query))\n",
        "\n",
        "        # Compute the feed forward\n",
        "        feed_forward_out = self.feed_forward(first_normalization)\n",
        "        # Make the second layer normalization (with residual connection)\n",
        "        return self.dropout(self.normalization_2(feed_forward_out + first_normalization))\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, num_layers, q_val: int, v_val: int, h_val: int, dropout):\n",
        "        \"\"\"\n",
        "        Builds the encoder part of a Transformer (a concatenation of TransformerBlocks)\n",
        "        :param num_layers: The number of TransformerBlocks to include in the encoder.\n",
        "        :param q_val: The q parameter of the Multi-Head-Attention block.\n",
        "        :param v_val: The v parameter of the Multi-Head-Attention block.\n",
        "        :param h_val: The h parameter of the Multi-Head-Attention block.\n",
        "        :param dropout: The percentage of dropout to place after the normalizations.\n",
        "        \"\"\"\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(num_layers):\n",
        "            self.layers.append(TransformerBlock(q_val, v_val, h_val, dropout))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "\n",
        "        :param x: Should have the following shape: (num_codebooks, f_r)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # Note:\n",
        "        # The input tokens are retrieved from the compression model, that have already imprinted the positional\n",
        "        # encoding. For this reason they can be passed directly to the transformer.\n",
        "\n",
        "        last_output = x\n",
        "        for layer in self.layers:\n",
        "            last_output = layer(last_output, last_output, last_output)\n",
        "\n",
        "        return last_output\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 q_val: int,\n",
        "                 v_val: int,\n",
        "                 h_val: int,\n",
        "                 dropout: float, ):\n",
        "        \"\"\"\n",
        "        :param q_val: The q parameter of the Multi-Head-Attention blocks.\n",
        "        :param v_val: The v parameter of the Multi-Head-Attention blocks.\n",
        "        :param h_val: The h parameter of the Multi-Head-Attention blocks.\n",
        "        :param dropout: The percentage of dropout to place after the normalizations.\n",
        "        \"\"\"\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.attention = MaskedSelfAttention(q_val, v_val)\n",
        "        self.normalization = nn.LayerNorm(v_val)\n",
        "        self.transformer_block = TransformerBlock(q_val, v_val, h_val, dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.cross_attention = MultiHeadCrossAttention(h_val, q_val, v_val)\n",
        "\n",
        "    def forward(self, x, value, key, src_mask, trg_mask, text_tokens=None):\n",
        "        \"\"\"\n",
        "        :param x: The input of the block.\n",
        "        :param value: The value matrix of the Self-Attention block, retrieved from the Transformer Encoder,\n",
        "        to pass to a Self-Attention block in the decoder.\n",
        "        :param key: The key matrix of the Self-Attention block, retrieved from the Transformer Encoder,\n",
        "        to pass to a Self-Attention block in the decoder.\n",
        "        :param src_mask: The mask passed to the encoder.\n",
        "        :param trg_mask: The mask to use in the decoder.\n",
        "        :param text_tokens: The embedded text to use for text-conditioning. Cross-validation will be computed\n",
        "         between text_tokens and the output of the block. If not provided, text-conditioning will not be performed.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        attention = self.attention(x, x, x, trg_mask)\n",
        "        normalization = self.normalization(attention + x)\n",
        "        transformer_block_output = self.transformer_block(normalization, key, value)\n",
        "        if text_tokens is None:\n",
        "            return transformer_block_output\n",
        "        else:\n",
        "            return self.cross_attention(transformer_block_output, text_tokens)\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, num_layers, q_val: int, v_val: int, h_val: int, dropout: float, embed_size: int,\n",
        "                 trg_vocab_size: int):\n",
        "        \"\"\"\n",
        "        Builds the decoder part of a Transformer (a concatenation of DecoderBlocks)\n",
        "        :param num_layers: The number of DecoderBlocks to include in the decoder.\n",
        "        :param q_val: The q parameter of the Multi-Head-Attention block.\n",
        "        :param v_val: The v parameter of the Multi-Head-Attention block.\n",
        "        :param h_val: The h parameter of the Multi-Head-Attention block.\n",
        "        :param dropout: The percentage of dropout to place after the normalizations.\n",
        "        :param embed_size: The size of the tokens passed to the Encoder.\n",
        "        :param trg_vocab_size: The number of outputs of the decoder. This can also be intended as the number of\n",
        "         output units of the decoder.\n",
        "        \"\"\"\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([DecoderBlock(q_val, v_val, h_val, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.full_conn_out = nn.Sequential(\n",
        "            nn.Linear(embed_size, trg_vocab_size),\n",
        "            nn.Softmax(dim=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, trg_mask, text_tokens=None):\n",
        "        \"\"\"\n",
        "\n",
        "        :param x:\n",
        "        :param encoder_output: The output of the Encoder Block\n",
        "        :param src_mask: The src_mask argument of the DecoderBlock\n",
        "        :param trg_mask: The trg_mask argument of the DecoderBlock\n",
        "        :param text_tokens: The embedded text to use for text-conditioning. If not provided, text-conditioning\n",
        "         will not be performed.\n",
        "        \"\"\"\n",
        "        curr_output = x\n",
        "        for dec_block in self.layers:\n",
        "            curr_output = dec_block(x, encoder_output, encoder_output, src_mask, trg_mask, text_tokens)\n",
        "\n",
        "        return self.full_conn_out(curr_output)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_layers, q_val: int, v_val: int, h_val: int, dropout: float, embed_size: int,\n",
        "                 trg_vocab_size: int, src_pad_idx: int):\n",
        "        \"\"\"\n",
        "        Builds a complete Transformer with an Encoder and a Decoder part\n",
        "        :param num_layers: The number of layers to include in the encoder and in the decoder.\n",
        "        :param q_val: The q parameter of the Multi-Head-Attention block.\n",
        "        :param v_val: The v parameter of the Multi-Head-Attention block.\n",
        "        :param h_val: The h parameter of the Multi-Head-Attention block.\n",
        "        :param dropout: The percentage of dropout to place after the normalizations.\n",
        "        :param embed_size: The size of the tokens passed to the Encoder.\n",
        "        :param trg_vocab_size: The number of outputs of the decoder. This can also be intended as the number of\n",
        "         output units of the decoder.\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_vocab_size = trg_vocab_size\n",
        "\n",
        "        self.encoder = TransformerEncoder(num_layers, q_val, v_val, h_val, dropout)\n",
        "        self.decoder = TransformerDecoder(num_layers, q_val, v_val, h_val, dropout, embed_size, trg_vocab_size)\n",
        "\n",
        "    @staticmethod\n",
        "    def make_mask(dim):\n",
        "        mask_ind = torch.tril(torch.ones((dim, dim), dtype=torch.bool), diagonal=-1).t()\n",
        "        mask = torch.tril(torch.ones(dim, dim))\n",
        "        mask[mask_ind] = sys.float_info.min\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def forward(self, enc_input, dec_input):\n",
        "        # Build masks for the encoder and for the decoder\n",
        "        enc_mask = self.make_mask(enc_input.shape[0])\n",
        "        dec_mask = self.make_mask(dec_input.shape[0])\n",
        "\n",
        "        encoder_output = self.encoder(enc_input)\n",
        "        return self.decoder(dec_input, encoder_output, enc_mask, dec_mask)\n",
        "\n",
        "\n",
        "class TransformerWithText(Transformer):\n",
        "    \"\"\"\n",
        "    A Transformer that accepts text conditioning\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, enc_input, dec_input, text_tokens):\n",
        "        # Build masks for the encoder and for the decoder\n",
        "        enc_mask = self.make_mask(enc_input.shape[0])\n",
        "        dec_mask = self.make_mask(dec_input.shape[0])\n",
        "\n",
        "        encoder_output = self.encoder(enc_input)\n",
        "        return self.decoder(dec_input, encoder_output, enc_mask, dec_mask, text_tokens)\n",
        "\n",
        "\n",
        "class TransformerWithTextAndMelody(TransformerWithText):\n",
        "    \"\"\"\n",
        "    Builds a Transformer that accepts both text and melody conditioning\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, enc_input, dec_input, text_tokens, melody_chromagram):\n",
        "        # Compute the conditioned input of the decoder\n",
        "        dec_input_conditioned = torch.concat((melody_chromagram, dec_input), dim=0)\n",
        "\n",
        "        # Build masks for the encoder and for the decoder\n",
        "        enc_mask = self.make_mask(enc_input.shape[0])\n",
        "        dec_mask = self.make_mask(dec_input_conditioned.shape[0])\n",
        "\n",
        "        encoder_output = self.encoder(enc_input)\n",
        "        return self.decoder(dec_input_conditioned, encoder_output, enc_mask, dec_mask, text_tokens)\n"
      ],
      "metadata": {
        "id": "4CcWoQg6qlq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transformer used in this project follows the architecture defined in \"Attention is all you need\" by Vaswani et al., apart from slight modifications.\n",
        "This is composed of an encoder and a decoder part with the same number of layers. Each layer of the encoder is a `TransformerBlock` (followed by a dropout layer), and consist in a Multi-Head Attention layer that is fed with the output of the embedding and positional encoding processes. The output of the attention layer (via a residual connection) is then normalized and passed to an MLP, where the hidden layer is 4 times the dimension of the input layer, that projects back to the initial dimension. The outcome of the output layer (via a residual connection) is, then, normalized and becomes the output of the `TransformerBlock`.\n",
        "Each layer of the decoder is made up of a first part, where the input (after embedding and positional encoding) is passed to a Masked-Self-Attention (in order to make the model causal) and then normalized, and a second part where we compute the Cross-Attention between the output of that normalization and the output of the encoder. The layer continues with the `TransformerBlock` as defined above. The output of the decoder is then projected to the desired dimension and passed to a Softmax function. The result of the activation will be the result of the decoder.\n",
        "The encoder and decoder are mixed up in the `Transformer` class.\n",
        "<br><br>\n",
        "This base structure is extended to support text and melody conditioning, as described by the reference paper.\n",
        "`TranformerWithText` allows to pass some tokens as input to the transfomer along with a text conditioning. The given text tokens are used at the end of each layer of the decoder to compute Cross-Attention with the output of the `TransformerBlock`. This becomes the new output of the decoder layer.\n",
        "While `TransformerWithTextAndMelody` allows to pass to the transformer audio tokens, some conditioning text and melody. The text tokens are used as for `TranformerWithText`, while the melody tokens (of the chromagram) are passed to the decoder, whose new input will be the concatenation of the melody and the audio tokens."
      ],
      "metadata": {
        "id": "KaYIZrHLSuGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The training proceess"
      ],
      "metadata": {
        "id": "gmtnGAz9W6_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from data_preparation import load_dataset\n",
        "from transformer import Transformer, TransformerWithText\n",
        "\n",
        "\n",
        "class TransformerTrainer:\n",
        "    def __init__(self, model: Transformer, model_save_dir):\n",
        "        \"\"\"\n",
        "\n",
        "        :param model: The model to train\n",
        "        :param model_save_dir: The directory where to save the trained model at the end of training.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.model_save_dir = model_save_dir\n",
        "\n",
        "    def train_on_sample(self, encoder_input, decoder_input, num_epochs: int, learning_rate: float,\n",
        "                        text_conditioning=None):\n",
        "        \"\"\"\n",
        "        :param text_conditioning: The sequence of tokens to use for text conditioning. It is\n",
        "        strictly related to the model passed to this class' constructor.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        # Define the loss function and the optimizer to use to train\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "\n",
        "        # Set the model in training mode\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # Reset the gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Calculate the output of the model\n",
        "            output = self.model(encoder_input, decoder_input, text_conditioning)\n",
        "            # Compute the current loss\n",
        "            loss = loss_fn(output, encoder_input)\n",
        "            # Compute the gradients\n",
        "            loss.backward()\n",
        "            # Perform an optimization step\n",
        "            optimizer.step()\n",
        "            print(f\"Epoch: {epoch + 1}, Loss: {loss.item()}\")\n",
        "\n",
        "    def train_on_sample_and_melody(self, encoder_input, decoder_input, num_epochs: int, learning_rate: float,\n",
        "                                   text_conditioning=None, melody_conditioning=None):\n",
        "        \"\"\"\n",
        "        :param text_conditioning: The sequence of tokens to use for text conditioning. It is\n",
        "        strictly related to the model passed to this class' constructor.\n",
        "        :param melody_conditioning: The sequence of tokens to use for melody conditioning. It is\n",
        "        strictly related to the model passed to this class' constructor.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        # Define the loss function and the optimizer to use to train\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "\n",
        "        # Set the model in training mode\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # Reset the gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Calculate the output of the model\n",
        "            output = self.model(encoder_input, decoder_input, text_conditioning, melody_conditioning)\n",
        "            # Compute the current loss\n",
        "            loss = loss_fn(output, torch.cat((melody_conditioning, encoder_input), dim=0))\n",
        "            # Compute the gradients\n",
        "            loss.backward()\n",
        "            # Perform an optimization step\n",
        "            optimizer.step()\n",
        "            print(f\"Epoch: {epoch + 1}, Loss: {loss.item()}\")\n",
        "\n",
        "    def train_on_dataset(self, dataset, num_epochs: int, learning_rate: float):\n",
        "        \"\"\"\n",
        "        Trains the model on the given dataset.\n",
        "        :param dataset: As returned from data_preparation.load_dataset().\n",
        "        \"\"\"\n",
        "        # Define the loss function and the optimizer to use to train\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "\n",
        "        # Set the model in training mode\n",
        "        self.model.train()\n",
        "\n",
        "        N = len(dataset)\n",
        "        for epoch in range(num_epochs):\n",
        "            # Reset the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            losses = torch.zeros(N)\n",
        "            for i in range(N):\n",
        "                # Get a sequence of tokens representing an audio file, and the associated text conditioning\n",
        "                audio = dataset[i]['audio']  # n1 x d\n",
        "                text = dataset[i]['text']  # n2 x d\n",
        "\n",
        "                enc_input = audio\n",
        "                dec_input = F.pad(input=enc_input[1:], pad=(0, 0, 1, 0), mode='constant', value=0)\n",
        "                # Calculate the output of the model\n",
        "                output = self.model(enc_input, dec_input, text)\n",
        "\n",
        "                # Compute and store the loss for the current sequence\n",
        "                curr_loss = loss_fn(output, enc_input).reshape(1)\n",
        "                losses[i] = curr_loss\n",
        "\n",
        "            # Compute the sum of the losses for all the samples\n",
        "            loss = torch.sum(losses)\n",
        "            # Compute the gradients\n",
        "            loss.backward()\n",
        "            # Perform an optimization step\n",
        "            optimizer.step()\n",
        "            print(f\"Epoch: {epoch + 1}, Loss: {loss.item()}\")\n",
        "\n",
        "        # Save the model\n",
        "        model_name = {round(time.time() * 1000)}\n",
        "        torch.save(self.model.state_dict(), f'{self.model_save_dir}/{model_name}')\n",
        "        torch.save(self.model, f'{self.model_save_dir}/{model_name}-2')"
      ],
      "metadata": {
        "id": "E4BAJkI2WtDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `TranformerTrainer` class is meant to train the model passed to the constructor, as long as this is a subclass of the `Transformer` defined in the previous section. It contains 2 main methods: `train_on_sample` (with the variant to allow melody conditioning) and `train_on_dataset`. As suggested by the name, the former trains the model for a certain number of epochs just on a single sample, and is meant just to show that the model is correctly working during the exam. While the latter trains the model on the whole given dataset, and is used to perform the real optimization of the cost function / of weights.\n",
        "The function used as loss is the CrossEntropy and the optimizer is Adam. After setting the model in training mode, at each iteration we reset the gradient and we pass all the sample sequences (one at a time) to the model. The whole sequence is passed to the encoder, while the shifted version is passed to the decoder, using a token of 0s as padding. Then, the loss between the reconstruction and the original input is computed for the current sequence. The loss of the current iteration will be the sum of all the losses. This will be used to compute the gradient, and the gradient will be used to update the weights (based on the learning rate).\n",
        "At the end of training we save the model at the given path (just for evaluation purposes)."
      ],
      "metadata": {
        "id": "9IotI8QZW_c9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The generation process"
      ],
      "metadata": {
        "id": "VuSm6LU5XACt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from Encodec import AudioTokenizer\n",
        "from data_preparation import load_dataset\n",
        "from transformer import TransformerWithText\n",
        "\n",
        "\n",
        "class AudioGenerator:\n",
        "    def __init__(self, model_path: str):\n",
        "        self.model = TransformerWithText(num_layers=5,\n",
        "                                         q_val=4,\n",
        "                                         v_val=4,\n",
        "                                         h_val=3,\n",
        "                                         dropout=0.1,\n",
        "                                         embed_size=4,  # 4,\n",
        "                                         trg_vocab_size=4,\n",
        "                                         src_pad_idx=0, )\n",
        "        self.model.load_state_dict(torch.load(model_path))\n",
        "        self.model.eval()  # set layers to evaluation mode\n",
        "\n",
        "        self.audio_tokenizer = AudioTokenizer()\n",
        "\n",
        "    def generate_audio(self, text, num_tokens: int = 1000, file_path=None):\n",
        "        \"\"\"\n",
        "        Returns the audio returned by the model when it is fed with random noise and the given text.\n",
        "        It returns the generated tokens and saves them as audio file based on whether file_path is None.\n",
        "        :param text: The description of the audio to generate\n",
        "        :param num_tokens: The number of tokens to generate.\n",
        "        :param file_path: The .wav file where to save the generated audio.\n",
        "        :return: the generated tokens and saves them as audio file based on whether file_path is None.\n",
        "        \"\"\"\n",
        "        rnd_noise1 = torch.rand((1, 4))\n",
        "        rnd_noise2 = torch.rand((1, 4))\n",
        "        mask = self.model.make_mask(1)\n",
        "\n",
        "        output = torch.empty((num_tokens, 4))\n",
        "        for i in range(num_tokens):\n",
        "            last_output = self.model.decoder(\n",
        "                x=output[i - 1:i] if i >= 1 else rnd_noise1,  # last generated token or random noise\n",
        "                encoder_output=output[i - 1:i] if i >= 1 else rnd_noise2,\n",
        "                src_mask=mask,\n",
        "                trg_mask=mask,\n",
        "                text_tokens=text,\n",
        "            )\n",
        "            output[i] = last_output\n",
        "\n",
        "        if file_path is not None:\n",
        "            self.audio_tokenizer.save_tokens_to_audio_file(output.t(), file_path)\n",
        ""
      ],
      "metadata": {
        "id": "w0ccWzO-izg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `AudioGenerator` class is meant to generate some audio from a text conditioning, using the pre-trained model provided in the constructor.\n",
        "The generation is performed using just the decoder and in an auto-regressive way: we start passing to the decoder two diffent tokens of random noise (along with text conditioning, the first one as input and the second as if it was the encoder output) to begin the generation process. Then, we continue passing the same text tokens, but we use the previously generated audio token as input, so that the new generation will be dependent on the description of the desired result and on the current partial generation. We store all the generated tokens and we either return them as returned from the decoder or we convert them to an audio .wav file.\n",
        "In order to be able to convert the generated tokens to audio we first have to turn vectors of float values to vectors of integers (as needed for the Encodec decoder). For this scope we first scale the values between 0 and 1, and then perform quantization to a predefined number of bins. The tokens are, then, ready to be converted to a .wav. So, the pre-trained model has been used to generate some audio related on the given textual description."
      ],
      "metadata": {
        "id": "6Wo2G6eWi56K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example of usage"
      ],
      "metadata": {
        "id": "2IPP0jXnpz43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: In the following example I will refer to the `load_dataset()` function, that is contained in the `data_preparation.py` file, which will not be posted in this notebook as relies on the presence of the dataset in the local disk, but can be found in the repository of our project."
      ],
      "metadata": {
        "id": "I2Py556CqLmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following code we are going to train the model described in the previous section. We will use the Musiccaps dataset but, since training a transformer on that amount of data would be unfeasible for our resources, we will use just a little slice composed of 32 samples.\n",
        "Using the `load_dataset()` function we load in memory the audio and caption associated with each sample, already converted to tokens using the `TextToTokenConverter` and the `AudioTokenizer` classes.\n",
        "Then, we initialize the Transformer with the choice of the hyperparameters that seems to lead to best results. Next, we initialize the trainer with the just created model and the path of the directory where we want to save the model after training.\n",
        "Using the `train_on_dataset()` method of the trainer we train the model for the desired number of epochs. To achieve good results we would need to train the model on a larger dataset and for a very big amount of time (~ a high number of epochs) but, for the same reason discussed above and regarding the available resources, this will not be possible to us. So we will just train for a small amount of epochs.\n",
        "The learning rate provided to the optimizer is $10^{-1}$, which is very big with respect to the commonly used learning rates. This choice is led by the fact that the starting loss of ~96k suggests us that we are very far from the optimal point of the cost function. In order to move faster towards it, we choose a high learning rate. Using the Adam optimizer it will then be adapted properly. This will cause us many problem if we get close to the optimal point, as we would not be able to stabilize on it but we would start oscillating on a certain interval centered on that point. However, as we can see from the training output, with a low number of epochs we stay very far from the optimal point and our choice of the learning rate successfully allows us to move as fast as possible towards the right direction, without ever missing the desired point and starting to get far from it."
      ],
      "metadata": {
        "id": "B3HnC7rGqr0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset()\n",
        "model = TransformerWithText(\n",
        "    num_layers=5,\n",
        "    q_val=4,\n",
        "    v_val=4,\n",
        "    h_val=3,\n",
        "    dropout=0.1,\n",
        "    embed_size=4,  # 4,\n",
        "    trg_vocab_size=4,\n",
        "    src_pad_idx=0,\n",
        ")\n",
        "trainer = TransformerTrainer(model, 'path-to-the-trained-model')\n",
        "trainer.train_on_dataset(dataset, num_epochs=2000, learning_rate=1e-1)"
      ],
      "metadata": {
        "id": "E7kepdJrwQVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "zdij26aWwzWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The orginal MusicGen model has been trained on a dataset containing **26.000 hours** of music, and **96 GPUs** allowed to find the optimal values for **3.3 billion weights** in a time in the magnitude order of **days** (as suggested by the \"Attention is all you need\" paper). So, this model is able to generate audio related to the provided description.\n",
        "However, as expected from the **Example of usage** section, with a low number of epochs and a small dataset, we are not able to train the model so that it can generate meaningful audio.\n",
        "Therefore, even if in the paper the CLAP score, KL divergence and FrÃ©chet Audio Distance are used to evaluate the model performance, along with subjective methods, we believe that conducting this kind of computations would not lead to results that are much different from listening to the generated audio to understant that it contains approximately random noise."
      ],
      "metadata": {
        "id": "_hjuwr9Gw1Uv"
      }
    }
  ]
}